import asyncio
import streamlit as st
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "embeddings" not in st.session_state:
    st.session_state.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

if "vector_store" not in st.session_state:
    st.session_state.vector_store = Chroma(persist_directory="./DB", embedding_function=st.session_state.embeddings)

if "conversation" not in st.session_state:
    st.session_state.conversation = []  # ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”

# ìŠ¤íƒ€ì¼ ì„¤ì •
st.image("./image_logo.png", width=300)
st.image("robot.gif")
st.markdown(
    """
    <div style="font-family: 'Arial', sans-serif; color:#000000; font-size:18px; font-weight:bold; text-align:center;">
        ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ë…¸ë¬´/ì¸ì‚¬ ê´€ë ¨í•˜ì—¬ ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”ğŸ¤–
    </div>
    """,
    unsafe_allow_html=True,
)
# ì¤„ ë„ìš°ê¸° ìœ„í•´ ë¹ˆ ì¤„ ì¶”ê°€ ë˜ëŠ” <br> ì‚¬ìš©
st.markdown("<br>", unsafe_allow_html=True)

st.markdown(
    """
    <div style="font-family: 'Arial', sans-serif; color:#000000; font-size:25px; font-weight:bold; text-align:center;">
        ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?
    </div>
    """,
    unsafe_allow_html=True
)

# Retriever ìƒì„±
retriever = st.session_state.vector_store.as_retriever(search_type="similarity", search_kwargs={'k': 5})

# ì‚¬ìš©ì ì…ë ¥
query = st.chat_input("ìƒë‹´ ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”.ì˜ˆ. ê·¼ë¡œê³„ì•½ì„œëŠ” ì–´ë–»ê²Œ ì¨ì•¼í•˜ë‚˜ìš”?")
st.markdown("<br>", unsafe_allow_html=True)
st.markdown("Lawbotì˜ ë‹µë³€ì´ ëª¨ë“  ìƒí™©ì— ì™„ë²½í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \në³´ë‹¤ ì •í™•í•œ ìƒë‹´ì´ í•„ìš”í•˜ì‹œë©´ ì „ë¬¸ê°€ì—ê²Œ ë¬¸ì˜í•˜ì‹œê¸¸ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤. ğŸ˜Š")

if query:

    # ì‚¬ìš©ìì˜ ì…ë ¥ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
    st.session_state.conversation.append({"role": "user", "content": query})

    # Spinnerë¥¼ í‘œì‹œí•˜ë©° ì‘ë‹µ ìƒì„±
    with st.spinner("Lawbotì´ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...ğŸ¤–"):
        try:
            # ë¬¸ì„œ ê²€ìƒ‰
            retrievals = retriever.batch([query])
            MAX_CONTEXT_LENGTH = 1600
            if retrievals and len(retrievals[0]) > 0:
                retrieved_context = "\n".join([doc.page_content[:MAX_CONTEXT_LENGTH] for doc in retrievals[0]])
            else:
                retrieved_context = "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ëŒ€í™” ìš”ì•½
            def summarize_conversation(conversation, model="gpt-4"):
                summary_prompt = ChatPromptTemplate.from_messages([
                    ("system", "ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”. ì¤‘ìš”í•œ ì •ë³´ë§Œ ìœ ì§€í•´ì£¼ì„¸ìš”. ë§¨ ì²˜ìŒì— 'ê¸°ì¡´ ëŒ€í™” ìš”ì•½'ì„ ì¨ì£¼ì„¸ìš”."),
                    ("user", conversation)
                ])
                chat_model = ChatOpenAI(model=model)
                output_parser = StrOutputParser()
                chain = summary_prompt | chat_model | output_parser
                summary = chain.invoke({"conversation": conversation})
                return summary

            # ê¸°ì¡´ ëŒ€í™” ìš”ì•½ ì¶”ê°€
            if st.session_state.conversation:
                summarized_conversation = summarize_conversation("\n".join([msg["content"] for msg in st.session_state.conversation]))
            else:
                summarized_conversation = ""

            # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            final_context = f"ê¸°ì¡´ ëŒ€í™” ìš”ì•½:\n{summarized_conversation}\n\nê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸:\n{retrieved_context}"

            # Prompt ìƒì„± ë° ì‘ë‹µ ì²˜ë¦¬
            prompt = ChatPromptTemplate.from_messages([
                ("system", '''
                1. í˜ë¥´ì†Œë‚˜ (Persona)
ë‹¹ì‹ ì€ í•œêµ­ ë…¸ë™ë²•ì— ì •í†µí•œ ë…¸ë™ë²• ì „ë¬¸ ë³€í˜¸ì‚¬ì´ë‹¤.
ì‚¬ìš©ìì—ê²Œ ë²•ì  ì¡°ì–¸ì„ ì œê³µí•˜ë©°, ë…¸ë™ë²• ë° ê´€ë ¨ íŒë¡€ì— ëŒ€í•œ ì „ë¬¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•œë‹¤.
ì¹œì ˆí•˜ê³  ì‹ ë¢°ê°ì„ ì£¼ëŠ” ë§íˆ¬ë¥¼ ìœ ì§€í•˜ë©°, ë²•ì  ì¡°ì–¸ê³¼ í•¨ê»˜ ì‹¤ì§ˆì ì¸ ì‹¤í–‰ ë°©ì•ˆë„ ì œì‹œí•œë‹¤.
2. ì—­í•  (Role)
ì‚¬ìš©ìë¡œë¶€í„° ë…¸ë™ë²• ê´€ë ¨ ì§ˆë¬¸ì´ë‚˜ ì‚¬ë¡€ë¥¼ ë°›ìœ¼ë©´, ê·¸ì— ëŒ€í•œ ë²•ë¥ ì  ê²€í† ì™€ ì¡°ì–¸ì„ ì œê³µí•œë‹¤.
í•œêµ­ ë…¸ë™ë²•ì˜ ì¡°í•­ ë° ì£¼ìš” íŒë¡€ë¥¼ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•œë‹¤.
í•„ìš”í•œ ê²½ìš°, ë¬¸ì„œ ì‘ì„± ê°€ì´ë“œ, ì†Œì†¡ ì ˆì°¨, ë˜ëŠ” ë…¸ì‚¬ ë¶„ìŸ í•´ê²°ì„ ìœ„í•œ ì „ëµì„ ì œì•ˆí•œë‹¤.
êµ¬ì–´ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•œë‹¤. 

4. ì¶œë ¥ í˜•ì‹ (Output Format)
êµ¬ì¡°í™”ëœ ë‹µë³€: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ëª…í™•íˆ ì´í•´í•˜ê³ , ê´€ë ¨ ë²• ì¡°í•­ ë° íŒë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì´ê³  ê°„ê²°í•œ ë‹µë³€ì„ ì œê³µí•œë‹¤.
1ë‹¨ê³„: ìš”ì•½: ì§ˆë¬¸ì˜ í•µì‹¬ ë‹¨ì–´ë‚˜ ì´ìŠˆì— ëŒ€í•´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ì§ˆë¬¸ì— ëŒ€í•œ ê°„ëµí•œ í•µì‹¬ ìš”ì•½ ì œê³µ.
2ë‹¨ê³„: ë²•ì  ê·¼ê±°: ê´€ë ¨ ë…¸ë™ë²• ì¡°í•­ ë° íŒë¡€ë¥¼ ì¸ìš©í•˜ë©°, ì‚¬ìš©ìì—ê²Œ ê·¼ê±°ë¥¼ ì„¤ëª….
3ë‹¨ê³„: ì‹¤í–‰ ë°©ì•ˆ: ì‚¬ìš©ì ìƒí™©ì— ì í•©í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë²•ì /í–‰ì •ì  ë°©ì•ˆì„ ì œì‹œ.
4ë‹¨ê³„: ì¶”ê°€ ì •ë³´ ì œê³µ: ì°¸ê³ í•  ë§Œí•œ ì™¸ë¶€ ìë£Œë‚˜ í•„ìš”í•œ ì¶”ê°€ ì¡°ì–¸ì„ ì•ˆë‚´.
'''),
                ("user", f"ì§ˆë¬¸: {query}\nì»¨í…ìŠ¤íŠ¸: {final_context}")
            ])

            chat_model = ChatOpenAI(model="gpt-4o")
            output_parser = StrOutputParser()
            chain = prompt | chat_model | output_parser

            response = chain.invoke({"query": query, "context": final_context})

            # ì‘ë‹µ ì¶”ê°€
            st.session_state.conversation.append({"role": "assistant", "content": response})

        except Exception as e:
            error_message = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            st.session_state.conversation.append({"role": "assistant", "content": error_message})

# ì´ì „ ëŒ€í™” ë Œë”ë§
for msg in st.session_state.conversation:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])
